{"cells":[{"cell_type":"markdown","metadata":{},"source":["# DATA ANALYSIS\n","In this section we gonna explore the available data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import spacy\n","import seaborn as sns\n","from collections import defaultdict\n","from collections import Counter\n","import numpy as np\n","import re\n","import seaborn as sns\n","\n","nlp = spacy.load('en_core_web_trf')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('data/train.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df[['target']].describe().transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Count the frequency of each target value\n","target_counts = df['target'].value_counts()\n","\n","# Plot the bar chart\n","plt.figure(figsize=(6, 4))\n","target_counts.plot(kind='bar')\n","plt.title('Frequency of Target Values')\n","plt.xlabel('Target Value')\n","plt.ylabel('Frequency')\n","plt.xticks(rotation=0)  # Rotate x-axis labels if needed\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['keyword'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Count NaN values in 'location' and 'keyword' columns\n","nan_location_count = df['location'].isna().sum()\n","nan_keyword_count = df['keyword'].isna().sum()\n","\n","# Count non-NaN values in 'location' and 'keyword' columns\n","non_nan_location_count = df['location'].notna().sum()\n","non_nan_keyword_count = df['keyword'].notna().sum()\n","\n","# Create labels for the pie charts\n","labels_location = [f'NaN \\n ({nan_location_count})', f'Non-NaN \\n ({non_nan_location_count})']\n","labels_keyword = [f'NaN \\n ({nan_keyword_count})', f'Non-NaN \\n ({non_nan_keyword_count})']\n","\n","# Create data for the pie charts\n","sizes_location = [nan_location_count, non_nan_location_count]\n","sizes_keyword = [nan_keyword_count, non_nan_keyword_count]\n","\n","# Create colors for different sections of the pie charts\n","colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n","\n","# Plot the pie chart for 'location' column\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.pie(sizes_location, labels=labels_location, colors=colors, autopct='%1.1f%%', startangle=140)\n","plt.axis('equal')\n","plt.title('Proportions of NaN values in Location')\n","\n","\n","# Plot the pie chart for 'keyword' column\n","plt.subplot(1, 2, 2)\n","plt.pie(sizes_keyword, labels=labels_keyword, colors=colors, autopct='%1.1f%%', startangle=140)\n","plt.axis('equal')\n","plt.title('Proportions of NaN Values in Keyword')\n","\n","# Adjust layout\n","plt.tight_layout()\n","\n","# Show the pie charts\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get unique locations with target equal to 1\n","unique_locations_target_one = df[df['target'] == 1]['location'].nunique()\n","\n","# Get unique locations with target equal to 0\n","unique_locations_target_zero = df[df['target'] == 0]['location'].nunique()\n","\n","# Plotting the histogram\n","plt.figure(figsize=(8, 6))\n","plt.bar(['Target=1', 'Target=0'], [unique_locations_target_one, unique_locations_target_zero], color=['#ff9999', '#66b3ff'])\n","plt.title('Number of Unique Locations with Target')\n","plt.xlabel('Target')\n","plt.ylabel('Number of Unique Locations')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get unique locations with target equal to 1\n","unique_keywords_target_one = df[df['target'] == 1]['keyword'].nunique()\n","\n","# Get unique locations with target equal to 0\n","unique_keywords_target_zero = df[df['target'] == 0]['keyword'].nunique()\n","\n","# Plotting the histogram\n","plt.figure(figsize=(8, 6))\n","plt.bar(['Target=1', 'Target=0'], [unique_keywords_target_one, unique_keywords_target_zero], color=['#ff9999', '#66b3ff'])\n","plt.title('Number of Unique Keywords with Target')\n","plt.xlabel('Target')\n","plt.ylabel('Number of Unique Keywords')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate the length of each text and create a new column 'text_length'\n","df['text_length'] = df['text'].apply(len)\n","\n","# Calculate the average and maximum text length\n","average_text_length = df['text_length'].mean()\n","max_text_length = df['text_length'].max()\n","\n","print(\"Average text length:\", average_text_length)\n","print(\"Maximum text length:\", max_text_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_text_len_targer = df.groupby('target')['text_length'].mean()\n","\n","print(average_text_len_targer)\n","\n","# Plot the average word count in a histogram\n","average_text_len_targer.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Text Length by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Text Lenght')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='text_length', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth=10)\n","plt.title('Distribution of Text Lenght Counts by Target')\n","plt.xlabel('Text Lenght')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to calculate word count\n","def word_count(text):\n","    return len(text.split())\n","\n","# Apply the function to each row of the 'text' column\n","df['word_count'] = df['text'].apply(lambda x: word_count(x))\n","\n","# Display the DataFrame\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate the average word count by target\n","average_word_count = df.groupby('target')['word_count'].mean()\n","print(average_word_count)\n","\n","# Plot the average word count in a histogram\n","average_word_count.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Word Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Word Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='word_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth=3)\n","plt.title('Distribution of WordCounts by Target')\n","plt.xlabel('Word Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Download NLTK stop words data\n","nltk.download('stopwords')\n","\n","# Load stop words\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to calculate stop word ratio\n","def stop_word_ratio(text):\n","    words = text.split()\n","    num_stop_words = sum(1 for word in words if word.lower() in stop_words)\n","    return num_stop_words / len(words) if len(words) > 0 else 0\n","\n","# Apply the function to each row of the 'text' column\n","df['stop_word_ratio'] = df['text'].apply(stop_word_ratio)\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate the average stop word ratio by target\n","average_stop_word_ratio = df.groupby('target')['stop_word_ratio'].mean()\n","print(average_stop_word_ratio)\n","\n","average_stop_word_ratio.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Stop Word Ratio by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Stop Word Ratio')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='stop_word_ratio', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.title('Distribution of Stop Words Ratio by Target')\n","plt.xlabel('Stops Word Ratio')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nltk.download('popular')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to calculate unique word count\n","def unique_word_count(text):\n","    words = word_tokenize(text.lower())  # Tokenize text and convert to lowercase\n","    unique_words = set(words)\n","    return len(unique_words)\n","\n","# Apply the function to each row of the 'text' column\n","df['unique_word_count'] = df['text'].apply(unique_word_count)\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate the average unique word count by target\n","average_unique_word_count = df.groupby('target')['unique_word_count'].mean()\n","\n","# Display the average unique word count\n","print(average_unique_word_count)\n","\n","average_unique_word_count.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Unique Word Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Unique Word Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='unique_word_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth=3)\n","plt.title('Distribution of Unique Words Count by Target')\n","plt.xlabel('Unique Word Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to calculate Type-Token Ratio (TTR)\n","def ttr(text):\n","    words = word_tokenize(text.lower())  # Tokenize text and convert to lowercase\n","    total_words = len(words)\n","    unique_words = set(words)\n","    total_unique_words = len(unique_words)\n","    if total_words > 0:\n","        return total_unique_words / total_words\n","    else:\n","        return 0\n","\n","# Apply the function to each row of the 'text' column\n","df['ttr'] = df['text'].apply(ttr)\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_ttr = df.groupby('target')['ttr'].mean()\n","\n","# Display the average ttr \n","print(average_ttr)\n","\n","average_ttr.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average TTR by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average TTR')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='ttr', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.title('Distribution of TTR by Target')\n","plt.xlabel('TTR')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to calculate Shannon entropy\n","def entropy(text):\n","    # Count the occurrences of each character in the text\n","    counts = Counter(text.lower())\n","    # Total number of characters\n","    total_chars = sum(counts.values())\n","    # Calculate the probability of each character\n","    probs = [count / total_chars for count in counts.values()]\n","    # Calculate Shannon entropy\n","    entropy = -np.sum([prob * np.log2(prob) for prob in probs])\n","    return entropy\n","\n","# Apply the function to each row of the 'text' column\n","df['entropy'] = df['text'].apply(entropy)\n","\n","# Display the DataFrame\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_entropy = df.groupby('target')['entropy'].mean()\n","\n","print(average_entropy)\n","\n","average_entropy.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Entropy by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Entropy')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='entropy', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.title('Distribution of Entropy by Target')\n","plt.xlabel('Entropy')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import string\n","\n","# Function to count punctuation symbols\n","def punctuation_count(text):\n","    # Define punctuation characters\n","    punctuation_chars = set(string.punctuation)\n","    # Count occurrences of punctuation symbols in the text\n","    punctuation_count = sum(1 for char in text if char in punctuation_chars)\n","    return punctuation_count\n","\n","# Apply the function to each row of the 'text' column\n","df['punctuation_count'] = df['text'].apply(punctuation_count)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_punctuation = df.groupby('target')['punctuation_count'].mean()\n","\n","print(average_punctuation)\n","\n","average_punctuation.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Punctation Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Punctuation Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(19, 10))\n","sns.displot(data= df, x='punctuation_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.xlim(0, 30)\n","plt.title('Distribution of Punctuation Count by Target')\n","plt.xlabel('Punctuation Count ')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to count named entities\n","def ner_count(text):\n","    doc = nlp(text)\n","    # Count unique named entities\n","    entities = set([ent.label_ for ent in doc.ents])\n","    return len(entities)\n","\n","# Apply the function to each row of the 'text' column\n","df['ner_count'] = df['text'].apply(ner_count)\n","\n","# Display the DataFrame\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_ner = df.groupby('target')['ner_count'].mean()\n","\n","print(average_ner)\n","\n","average_ner.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average NER Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average NER Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='ner_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], discrete = True)\n","plt.xlim(0, 5)\n","plt.title('Distribution of NER Count by Target')\n","plt.xlabel('NER Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to count POS tags\n","def pos_count(text, pos_tags):\n","    doc = nlp(text)\n","    pos_counts = 0\n","    for token in doc:\n","        if token.pos_ in pos_tags:\n","            pos_counts += 1\n","    return pos_counts\n","\n","# Define the POS tags you want to count\n","pos_tags = ['NOUN', 'ADJ', 'VERB', 'ADP', 'PRON']\n","\n","# Apply the function to each row of the 'text' column\n","df['tot_pos_counts'] = df['text'].apply(lambda x: pos_count(x, pos_tags))\n","df['NOUN_counts'] = df['text'].apply(lambda x: pos_count(x, ['NOUN']))\n","df['ADJ_counts'] = df['text'].apply(lambda x: pos_count(x, ['ADJ']))\n","df['VERB_counts'] = df['text'].apply(lambda x: pos_count(x, ['VERB']))\n","df['ADP_counts'] = df['text'].apply(lambda x: pos_count(x, ['ADP']))\n","df['PRON_counts'] = df['text'].apply(lambda x: pos_count(x, ['PRON']))\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('analyzed_train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_tot_pos = df.groupby('target')['tot_pos_counts'].mean()\n","\n","print(average_tot_pos)\n","\n","average_tot_pos.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Total POS Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Total POS Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='tot_pos_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.title('Distribution of Total POS Count by Target')\n","plt.xlabel('POS Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_nouns = df.groupby('target')['NOUN_counts'].mean()\n","\n","print(average_nouns)\n","\n","average_nouns.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average NOUN Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average NOUN Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='NOUN_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n","plt.xlim(0, 15)\n","plt.title('Distribution of NOUN Count by Target')\n","plt.xlabel('NOUN Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_adj = df.groupby('target')['ADJ_counts'].mean()\n","\n","print(average_adj)\n","\n","average_adj.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average ADJ Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average ADJ Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='ADJ_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n","plt.xlim(0, 6)\n","plt.title('Distribution of ADJ Count by Target')\n","plt.xlabel('ADJ Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_verb = df.groupby('target')['VERB_counts'].mean()\n","\n","print(average_verb)\n","\n","average_verb.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average VERB Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average VERB Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='VERB_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n","plt.xlim(0, 7)\n","plt.title('Distribution of VERB Count by Target')\n","plt.xlabel('VERB Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_adp = df.groupby('target')['ADP_counts'].mean()\n","\n","print(average_adp)\n","\n","average_adp.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average ADP Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average ADP Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='ADP_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n","plt.xlim(0, 6)\n","plt.title('Distribution of ADP Count by Target')\n","plt.xlabel('ADP Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_pron = df.groupby('target')['PRON_counts'].mean()\n","\n","print(average_pron)\n","\n","average_pron.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average PRON Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average PRON Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='PRON_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n","plt.xlim(0, 6)\n","plt.title('Distribution of PRON Count by Target')\n","plt.xlabel('PRON Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to count subordinate clauses\n","def count_subordinate_clauses(text):\n","    doc = nlp(text)\n","    # Initialize counter for subordinate clauses\n","    subordinate_clause_count = 0\n","    # Iterate over the tokens in the document\n","    for token in doc:\n","        # Check if the token is a subordinating conjunction or a dependent marker\n","        if token.dep_ in ['mark', 'advcl', 'ccomp']:\n","            subordinate_clause_count += 1\n","    return subordinate_clause_count\n","\n","# Apply the function to each row of the 'text' column\n","df['subordinate_clause_count'] = df['text'].apply(count_subordinate_clauses)\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_subordinate = df.groupby('target')['subordinate_clause_count'].mean()\n","\n","print(average_subordinate)\n","\n","average_subordinate.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Subordinate Clauses by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Subordinate Clauses')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='subordinate_clause_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.xlim(0, 6)\n","plt.title('Distribution of Subordinate Clause Count by Target')\n","plt.xlabel('Subordinate Clause Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to count passive constructions\n","def count_passive_constructions(text):\n","    doc = nlp(text)\n","    # Initialize counter for passive constructions\n","    passive_construction_count = 0\n","    # Iterate over the tokens in the document\n","    for token in doc:\n","        # Check if the token is a verb in passive voice (identified by its dependency relation)\n","        if token.dep_ == 'pass' and token.head.pos_ == 'VERB':\n","            passive_construction_count += 1\n","    return passive_construction_count\n","\n","# Apply the function to each row of the 'text' column\n","df['passive_construction_count'] = df['text'].apply(count_passive_constructions)\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_passive = df.groupby('target')['passive_construction_count'].mean()\n","\n","print(average_passive)\n","\n","average_passive.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Passive Constructions by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Passive Constructions')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer, AutoConfig\n","from tqdm.notebook import tqdm\n","from scipy.special import softmax\n","import numpy as np\n","\n","# Preprocess text (username and link placeholders)\n","def preprocess(text):\n","    new_text = []\n","    for t in text.split(\" \"):\n","        t = '@user' if t.startswith('@') and len(t) > 1 else t\n","        t = 'http' if t.startswith('http') else t\n","        new_text.append(t)\n","    return \" \".join(new_text)\n","MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","config = AutoConfig.from_pretrained(MODEL)\n","# PT\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n","\n","def get_scores(scores):\n","    score = dict()\n","    ranking = np.argsort(scores)\n","    ranking = ranking[::-1]\n","    for i in range(scores.shape[0]):\n","        l = config.id2label[ranking[i]]\n","        s = scores[ranking[i]]\n","        score[l] = np.round(float(s), 4)\n","\n","    return score\n","\n","\n","df[\"positive\"] = 0\n","df[\"neutral\"] = 0\n","df[\"negative\"] = 0\n","\n","\n","for i in tqdm(df.index):\n","    text = df[\"text\"][i]\n","    text = preprocess(text)\n","    encoded_input = tokenizer(text, return_tensors='pt')\n","    output = model(**encoded_input)\n","    scores = output[0][0].detach().numpy()\n","    scores = softmax(scores)\n","    dict_scores = get_scores(scores)\n","    df[\"positive\"][i] = dict_scores[\"positive\"]\n","    df[\"neutral\"][i] = dict_scores[\"neutral\"]\n","    df[\"negative\"][i] = dict_scores[\"negative\"]\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('analyzed_train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate average sentiment scores for each target group\n","average_scores = df.groupby('target')[['positive', 'neutral', 'negative',]].mean()\n","\n","# Plot the average sentiment scores\n","average_scores.plot(kind='bar', figsize=(8, 6))\n","plt.xlabel('Target')\n","plt.ylabel('Average Sentiment Score')\n","plt.title('Average Sentiment Score by Target')\n","plt.xticks([0, 1], ['Target 0', 'Target 1'], rotation=0)\n","plt.legend(title='Sentiment')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to count numbers\n","def count_numbers(text):\n","    numbers = sum(c.isdigit() for c in text)\n","    return numbers\n","\n","# Apply the function to each row of the 'text' column\n","df['number_count'] = df['text'].apply(count_numbers)\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate the average number count by target\n","average_number_count = df.groupby('target')['number_count'].mean()\n","\n","print(average_number_count)\n","\n","# Plot the average number count\n","average_number_count.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Number Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Number Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='number_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 2)\n","plt.xlim(0, 20)\n","plt.title('Distribution of Number Count by Target')\n","plt.xlabel('Number Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["N = 100\n","\n","STOPWORDS = set(stopwords.words('english'))\n","\n","def generate_ngrams(text, n_gram=1):\n","    tokens = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n","    ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n","    return [' '.join(ngram) for ngram in ngrams]\n","        \n","# Unigrams\n","disaster_unigrams = defaultdict(int)\n","nondisaster_unigrams = defaultdict(int)\n","\n","for tweet in df[df['target'] == 1]['text']:\n","    for word in generate_ngrams(tweet):\n","        disaster_unigrams[word] += 1\n","        \n","for tweet in df[df['target'] == 0]['text']:\n","    for word in generate_ngrams(tweet):\n","        nondisaster_unigrams[word] += 1\n","        \n","df_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\n","df_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n","\n","# Bigrams\n","disaster_bigrams = defaultdict(int)\n","nondisaster_bigrams = defaultdict(int)\n","\n","for tweet in df[df['target'] == 1]['text']:\n","    for word in generate_ngrams(tweet, n_gram=2):\n","        disaster_bigrams[word] += 1\n","        \n","for tweet in df[df['target'] == 0]['text']:\n","    for word in generate_ngrams(tweet, n_gram=2):\n","        nondisaster_bigrams[word] += 1\n","        \n","df_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\n","df_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n","\n","# Trigrams\n","disaster_trigrams = defaultdict(int)\n","nondisaster_trigrams = defaultdict(int)\n","\n","for tweet in df[df['target'] == 1]['text']:\n","    for word in generate_ngrams(tweet, n_gram=3):\n","        disaster_trigrams[word] += 1\n","        \n","for tweet in df[df['target'] == 0]['text']:\n","    for word in generate_ngrams(tweet, n_gram=3):\n","        nondisaster_trigrams[word] += 1\n","        \n","df_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\n","df_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n","plt.tight_layout()\n","\n","sns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='blue')\n","sns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='orange')\n","\n","for i in range(2):\n","    axes[i].spines['right'].set_visible(False)\n","    axes[i].set_xlabel('')\n","    axes[i].set_ylabel('')\n","    axes[i].tick_params(axis='x', labelsize=13)\n","    axes[i].tick_params(axis='y', labelsize=13)\n","\n","axes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)\n","axes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n","plt.tight_layout()\n","\n","sns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='blue')\n","sns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='orange')\n","\n","for i in range(2):\n","    axes[i].spines['right'].set_visible(False)\n","    axes[i].set_xlabel('')\n","    axes[i].set_ylabel('')\n","    axes[i].tick_params(axis='x', labelsize=13)\n","    axes[i].tick_params(axis='y', labelsize=13)\n","\n","axes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\n","axes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(ncols=2, figsize=(20, 50), dpi=100)\n","\n","sns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='blue')\n","sns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='orange')\n","\n","for i in range(2):\n","    axes[i].spines['right'].set_visible(False)\n","    axes[i].set_xlabel('')\n","    axes[i].set_ylabel('')\n","    axes[i].tick_params(axis='x', labelsize=13)\n","    axes[i].tick_params(axis='y', labelsize=11)\n","\n","axes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)\n","axes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('analyzed_train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to count URLs in a given text\n","def count_urls(text):\n","    url_pattern = r'(https?://)?(www\\.)?\\S+\\.\\S+'\n","    urls = re.findall(url_pattern, text)\n","    return len(urls)\n","\n","# Applying the function to the 'text' column\n","df['url_count'] = df['text'].apply(count_urls)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_urls = df.groupby('target')['url_count'].mean()\n","\n","print(average_urls)\n","\n","average_urls.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average URLs Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average URLs Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='url_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n","plt.title('Distribution of URL Count by Target')\n","plt.xlabel('URL Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def count_hashtags(text):\n","    hashtag_pattern = r'#\\w+'\n","    hashtags = re.findall(hashtag_pattern, text)\n","    return len(hashtags)\n","\n","# Applying the function to the 'text' column\n","df['hashtag_count'] = df['text'].apply(count_hashtags)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["average_hashtags = df.groupby('target')['hashtag_count'].mean()\n","\n","print(average_hashtags)\n","\n","average_hashtags.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Average Hashtags Count by Target')\n","plt.xlabel('Target')\n","plt.ylabel('Average Hashtags Count')\n","plt.xticks(rotation=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the distribution\n","sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10, 6))\n","sns.displot(data= df, x='hashtag_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n","plt.xlim(0, 8)\n","plt.title('Distribution of Hashtag Count by Target')\n","plt.xlabel('Hashtag Count')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.to_csv('analyzed_train.csv')\n"]},{"cell_type":"markdown","metadata":{},"source":["# DATA PROCESSING"]},{"cell_type":"markdown","metadata":{},"source":["In this section we process the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","from geopy.geocoders import Nominatim\n","from matplotlib import pyplot as plt\n","from wordcloud import WordCloud\n","from spacy.tokens import Doc\n","from typing import List\n","from tqdm import tqdm\n","import pandas as pd\n","import spacy\n","import math\n","import json\n","import re\n","import os\n","\n","source_data_path = '../data/train.csv'\n","processed_data_path = '../cleaned_train.csv'\n","\n","df = pd.read_csv(source_data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SpaCyPreProcessor:\n","\n","    def __init__(self, spacy_model = None, remove_numbers = False, remove_special = False, pos_to_remove = None, remove_stop_words = False, lemmatize = False, use_gpu = False) -> None:\n","        \n","        self.__remove_numbers = remove_numbers\n","        self.__remove_special = remove_special\n","        self.__pos_to_remove = pos_to_remove\n","        self.__remove_stop_words = remove_stop_words\n","        self.__lemmatize = lemmatize\n","\n","        if spacy_model is None:\n","            self.model = spacy.load(\"en_core_web_sm\")\n","        else:\n","            self.model = spacy_model\n","\n","        if use_gpu:\n","            spacy.prefer_gpu()\n","\n","    @staticmethod\n","    def download_spacy_model(model=\"en_core_web_sm\"):\n","        print(f\"Downloading spaCy model {model}\")\n","        spacy.cli.download(model)\n","        print(f\"Finished downloading model\")\n","\n","    @staticmethod\n","    def load_model(model=\"en_core_web_sm\"):\n","        return spacy.load(model, disable=[\"ner\", \"parser\"])\n","\n","    def tokenize(self, text) -> List[str]:\n","        \"\"\"\n","        Tokenize text using a spaCy pipeline\n","        :param text: Text to tokenize\n","        :return: list of str\n","        \"\"\"\n","        doc = self.model(text)\n","        return [token.text for token in doc]\n","\n","    def preprocess_text(self, text) -> str:\n","        \"\"\"\n","        Runs a spaCy pipeline and removes unwanted parts from text\n","        :param text: text string to clean\n","        :return: str, clean text\n","        \"\"\"\n","        doc = self.model(text)\n","        return self.__clean(doc)\n","\n","    def preprocess_text_list(self, texts=List[str]) -> List[str]:\n","        \"\"\"\n","        Runs a spaCy pipeline and removes unwantes parts from a list of text.\n","        Leverages spaCy's `pipe` for faster batch processing.\n","        :param texts: List of texts to clean\n","        :return: List of clean texts\n","        \"\"\"\n","        clean_texts = []\n","        for doc in tqdm(self.model.pipe(texts)):\n","            clean_texts.append(self.__clean(doc))\n","\n","        return clean_texts\n","\n","    def __clean(self, doc: Doc) -> str:\n","\n","        tokens = []\n","        # POS Tags removal\n","        if self.__pos_to_remove:\n","            for token in doc:\n","                if token.pos_ not in self.__pos_to_remove:\n","                    tokens.append(token)\n","        else:\n","            tokens = doc\n","\n","        # Remove Numbers\n","        if self.__remove_numbers:\n","            tokens = [\n","                token for token in tokens if not (token.like_num or token.is_currency)\n","            ]\n","\n","        # Remove Stopwords\n","        if self.__remove_stop_words:\n","            tokens = [token for token in tokens if not token.is_stop]\n","        # remove unwanted tokens\n","        tokens = [\n","            token\n","            for token in tokens\n","            if not (\n","                token.is_punct or token.is_space or token.is_quote or token.is_bracket\n","            )\n","        ]\n","\n","        # Remove empty tokens\n","        tokens = [token for token in tokens if token.text.strip() != \"\"]\n","\n","        # Lemmatize\n","        if self.__lemmatize:\n","            text = \" \".join([token.lemma_ for token in tokens])\n","        else:\n","            text = \" \".join([token.text for token in tokens])\n","\n","        if self.__remove_special:\n","            # Remove non alphabetic characters\n","            text = re.sub(r\"[^a-zA-Z\\']\", \" \", text)\n","        # remove non-Unicode characters\n","        text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n","\n","        text = text.lower()\n","\n","        return text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","keywords = list(set(df[df['keyword'].notna()]['keyword']))\n","keywords_lemmatized = [lemmatizer.lemmatize(word).lower() for word in keywords]\n","filtered_df = df[df['keyword'].isna()]\n","texts_without_keyword = filtered_df['text']\n","ids_without_keyword = filtered_df['id']\n","pattern = r'\\#\\w+'\n","extracted_words = []\n","\n","for id, text in zip(ids_without_keyword, texts_without_keyword):\n","    matches = re.findall(pattern, text)\n","    for match in matches:\n","        word = re.search(r'\\w+', match).group()\n","        if word in keywords or lemmatizer.lemmatize(word).lower() in keywords_lemmatized:\n","            row_index = df.index[df['id'] == id].tolist()[0]\n","            df['keyword'][row_index] = word\n","            extracted_words.append(word)\n","        else:\n","            keywords = df.loc[df['keyword'].notna() & df['text'].str.contains(word), 'keyword'].tolist()\n","            if len(keywords) == 1:\n","                row_index = df.index[df['id'] == id].tolist()[0]\n","                df['keyword'][row_index] = keywords[0]\n","df.loc[df['keyword'].isna(), 'keyword'] = 'NONE'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SpaCyPreProcessor.download_spacy_model('en_core_web_trf')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["spacy_model = SpaCyPreProcessor.load_model('en_core_web_trf')\n","preprocessing_pipeline = SpaCyPreProcessor(spacy_model=spacy_model, remove_numbers=False, remove_special=True, remove_stop_words=True, lemmatize=True, use_gpu=True)\n","\n","df['cleaned_text'] = ''\n","df['cleaned_keyword'] = ''\n","\n","for i in tqdm(df.index):\n","    df['cleaned_text'][i] = preprocessing_pipeline.preprocess_text(df['text'][i])\n","    df['cleaned_keyword'][i] = preprocessing_pipeline.preprocess_text(df['keyword'][i])\n","df.loc[df['cleaned_keyword'] == '', 'cleaned_keyword'] = 'none'\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.to_csv(processed_data_path)"]},{"cell_type":"markdown","metadata":{},"source":["### we search country for all locations to group tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(processed_data_path)\n","nlp = spacy.load('en_core_web_trf')\n","locations = df[df['location'].notna()]['location'].tolist()\n","results = None\n","arg = (locations, 0, len(locations))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Geolocator:\n","\n","    def __init__(self):\n","        self.geolocator = Nominatim(user_agent=f\"track\")\n","\n","    def get_country(self, location):\n","        country = 'nan'\n","        if location != 'nan':\n","            geocode = self.geolocator.geocode(location)\n","            if geocode:\n","                country = geocode.address.split(',')[-1]\n","        return country\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def update_countries_found(location, country, countries_found = []):\n","    check = False\n","    for i in range(len(countries_found)):\n","        if countries_found[i][\"country\"] == country:\n","            countries_found[i][\"locations\"].append(location)\n","            check = True\n","    if check == False:\n","        countries_found.append({\n","            \"locations\": [location],\n","            \"country\": country\n","        })\n","    return countries_found"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_countries(locations, start, end, countries_found = None):\n","    if countries_found == None:\n","        countries_found = []\n","    index = 0\n","    cicle = None\n","    try:\n","        geolocator = Geolocator()\n","        cicle = range(start, end) if end < len(locations) else range(start, len(locations))\n","        for i in tqdm(cicle):\n","            country = geolocator.get_country(str(locations[i]))\n","            countries_found = update_countries_found(locations[i], country, countries_found)\n","            index = i\n","        return countries_found, end, end\n","    except Exception as e:\n","        print(e)\n","        return countries_found, index, end"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_path = \"./locations_checkpoint.json\"\n","\n","if os.path.isfile(checkpoint_path):\n","    with open(checkpoint_path, 'r') as openfile:\n","        results = json.load(openfile)\n","\n","results = get_countries(*arg, results[0] if results else None)\n","if int(results[-2]) != int(results[-1]):\n","    arg = (locations, int(results[-2]), int(results[-1]))\n","\n","with open(checkpoint_path, 'w') as openfile:\n","    json.dump(results, openfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if os.path.isfile(checkpoint_path):\n","    with open(checkpoint_path, 'r') as openfile:\n","        results = json.load(openfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create a new column named country\n","countries = []\n","for i in range(len(df['location'])):\n","    countries_found = results[0]\n","    location = str(df.iloc[i]['location'])\n","    countries_found = str(None)\n","    for country in countries_found:\n","        if location in country['locations']:\n","            country_found = country['country']\n","    countries.append(country_found)\n","df2 = df.assign(country=countries)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["countries = list(set(df2['country'].tolist()))\n","countries = [country.replace(' ', '_') for country in countries]\n","text = \" \".join(countries)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wordcloud = WordCloud(background_color=\"white\", ).generate(text)\n","wordcloud.to_file(\"wordcloud_country.png\")\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.to_csv(processed_data_path)"]},{"cell_type":"markdown","metadata":{},"source":["# BERT/ROBERTA/DISTILBERT FINETUNING"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:45:06.886232Z","iopub.status.busy":"2024-06-01T18:45:06.885865Z","iopub.status.idle":"2024-06-01T18:45:47.482224Z","shell.execute_reply":"2024-06-01T18:45:47.480724Z","shell.execute_reply.started":"2024-06-01T18:45:06.886201Z"},"trusted":true},"outputs":[],"source":["!pip install -q transformers datasets\n","!pip install accelerate -U -q\n","!pip install wandb -q"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:45:50.608612Z","iopub.status.busy":"2024-06-01T18:45:50.607801Z","iopub.status.idle":"2024-06-01T18:45:51.688629Z","shell.execute_reply":"2024-06-01T18:45:51.687096Z","shell.execute_reply.started":"2024-06-01T18:45:50.608568Z"},"trusted":true},"outputs":[],"source":["!rm -rf ./outs ./wandb"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:45:54.560873Z","iopub.status.busy":"2024-06-01T18:45:54.560445Z","iopub.status.idle":"2024-06-01T18:46:02.917786Z","shell.execute_reply":"2024-06-01T18:46:02.916980Z","shell.execute_reply.started":"2024-06-01T18:45:54.560833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-01 18:45:59.603743: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-01 18:45:59.603801: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-01 18:45:59.605511: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel\n","from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from typing import Optional, Union, Tuple\n","from torch.optim import AdamW\n","from datasets import Dataset\n","from torch import nn\n","import pandas as pd\n","import numpy as np\n","import torch\n","import wandb\n","import os\n","\n","model = \"DISTILBERT\"\n","os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\n","os.environ[\"WANDB_API_KEY\"] = \"bd3ba2883541ec7f7d48ba92a4e37de6d56c1a94\"  # log all model checkpoints"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:46:11.530243Z","iopub.status.busy":"2024-06-01T18:46:11.529823Z","iopub.status.idle":"2024-06-01T18:46:11.540316Z","shell.execute_reply":"2024-06-01T18:46:11.539375Z","shell.execute_reply.started":"2024-06-01T18:46:11.530192Z"},"trusted":true},"outputs":[],"source":["class ClassificationHead(nn.Module):\n","    \"\"\"Head for sentence-level classification tasks.\"\"\"\n","\n","    def __init__(self, config, num_extra_dims):\n","        super().__init__()\n","        total_dims = config.hidden_size+num_extra_dims\n","        self.dense = nn.Linear(total_dims, total_dims)\n","        classifier_dropout = (\n","            config.dropout if config.dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.out_proj = nn.Linear(total_dims, config.num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = self.dropout(features)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:46:14.908212Z","iopub.status.busy":"2024-06-01T18:46:14.907632Z","iopub.status.idle":"2024-06-01T18:46:14.927086Z","shell.execute_reply":"2024-06-01T18:46:14.925868Z","shell.execute_reply.started":"2024-06-01T18:46:14.908179Z"},"trusted":true},"outputs":[],"source":["class CustomDistilBertForSequenceClassification(DistilBertForSequenceClassification):\n","\n","    def __init__(self, config, num_extra_dims):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","\n","        # might need to rename this depending on the model\n","        self.distilbert =  DistilBertModel(config)\n","        self.classifier = ClassificationHead(config, num_extra_dims)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    \n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        extra_data: Optional[torch.FloatTensor] = None,\n","        #token_type_ids: Optional[torch.LongTensor] = None,\n","        #position_ids: Optional[torch.LongTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, SequenceClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.distilbert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            #token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        # sequence_output will be (batch_size, seq_length, hidden_size)\n","        sequence_output = outputs[0]\n","\n","        # additional data should be (batch_size, num_extra_dims)\n","        cls_embedding = sequence_output[:, 0, :]\n","\n","        output = torch.cat((cls_embedding, extra_data), dim=-1)\n","\n","        logits = self.classifier(output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = nn.MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = nn.BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CustomRobertaForSequenceClassification(RobertaForSequenceClassification):\n","\n","    def __init__(self, config, num_extra_dims):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","\n","        # might need to rename this depending on the model\n","        self.roberta =  RobertaModel(config)\n","        self.classifier = ClassificationHead(config, num_extra_dims)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    \n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        extra_data: Optional[torch.FloatTensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, SequenceClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        # sequence_output will be (batch_size, seq_length, hidden_size)\n","        sequence_output = outputs[0]\n","\n","        # additional data should be (batch_size, num_extra_dims)\n","        cls_embedding = sequence_output[:, 0, :]\n","\n","        output = torch.cat((cls_embedding, extra_data), dim=-1)\n","\n","        logits = self.classifier(output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = nn.MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = nn.BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CustomBertForSequenceClassification(BertForSequenceClassification):\n","\n","    def __init__(self, config, num_extra_dims):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","\n","        # might need to rename this depending on the model\n","        self.bert =  BertModel(config)\n","        self.classifier = ClassificationHead(config, num_extra_dims)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    \n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        extra_data: Optional[torch.FloatTensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, SequenceClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        # sequence_output will be (batch_size, seq_length, hidden_size)\n","        sequence_output = outputs[0]\n","\n","        # additional data should be (batch_size, num_extra_dims)\n","        cls_embedding = sequence_output[:, 0, :]\n","\n","        output = torch.cat((cls_embedding, extra_data), dim=-1)\n","\n","        logits = self.classifier(output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = nn.MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = nn.BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:46:23.447466Z","iopub.status.busy":"2024-06-01T18:46:23.446687Z","iopub.status.idle":"2024-06-01T18:46:23.789042Z","shell.execute_reply":"2024-06-01T18:46:23.787939Z","shell.execute_reply.started":"2024-06-01T18:46:23.447422Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of CustomDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["new_model = CustomDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3, num_extra_dims=5)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:47:45.721307Z","iopub.status.busy":"2024-06-01T18:47:45.720318Z","iopub.status.idle":"2024-06-01T18:47:47.931099Z","shell.execute_reply":"2024-06-01T18:47:47.929940Z","shell.execute_reply.started":"2024-06-01T18:47:45.721266Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm-mattiello2\u001b[0m (\u001b[33mhlt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n","if model == \"BERT\":\n","    os.environ[\"WANDB_PROJECT\"] = \"BERT-Finetuning\"  # name your W&B project\n","    model_id = \"bert-base-uncased\"\n","    tokenizer = BertTokenizer.from_pretrained(model_id)\n","    BERTModel = BertForSequenceClassification\n","\n","elif model == \"ROBERTA\":\n","    os.environ[\"WANDB_PROJECT\"] = \"ROBERTA-Finetuning\"  # name your W&B project\n","    model_id = \"roberta-base\"\n","    tokenizer = RobertaTokenizer.from_pretrained(model_id)\n","    BERTModel = RobertaForSequenceClassification\n","\n","elif model == \"DISTILBERT\":\n","    os.environ[\"WANDB_PROJECT\"] = \"DISTILBERT-Finetuning\"  # name your W&B project\n","    model_id = \"distilbert-base-uncased\"\n","    tokenizer = DistilBertTokenizer.from_pretrained(model_id)\n","    BERTModel = CustomDistilBertForSequenceClassification\n","\n","else:\n","    raise Exception(\"No valid model specified\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:47:59.201789Z","iopub.status.busy":"2024-06-01T18:47:59.201009Z","iopub.status.idle":"2024-06-01T18:47:59.211656Z","shell.execute_reply":"2024-06-01T18:47:59.210667Z","shell.execute_reply.started":"2024-06-01T18:47:59.201748Z"},"trusted":true},"outputs":[],"source":["dataset_opts = [\n","    {\n","        'path': '../input/disaster-tweets-hlt/preprocess_one.csv',\n","        'feature': False,\n","    },\n","    {\n","        'path': '../input/disaster-tweets-hlt/preprocess_two.csv',\n","        'feature': False,\n","    },\n","        {\n","        'path': '../input/disaster-tweets-hlt/preprocess_three.csv',\n","        'feature': True,\n","    },\n","        {\n","        'path': '../input/disaster-tweets-hlt/preprocess_four.csv',\n","        'feature': True,\n","    }\n","]\n","# Sweep configuration\n","sweep_configuration = {\n","    \"method\": \"bayes\",\n","    \"name\": \"sweep\",\n","    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval/f1\"},\n","    \"parameters\": {\n","        \"batch_size\": {\"values\": [64]},\n","        \"epochs\": {\"values\": [10]},\n","        \"lr\": {\"max\": 0.0001, \"min\": 0.00001},\n","        \"w_decay\": {\"max\": 0.01, \"min\": 0.001},\n","        \"dataset\": {\"values\": [3, 4]},\n","        \"num_layers\": {\"values\": [12, 14, 16]},\n","        \"dp\": {\"max\": 0.75, \"min\": 0.45},\n","    },\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:48:02.312562Z","iopub.status.busy":"2024-06-01T18:48:02.311504Z","iopub.status.idle":"2024-06-01T18:48:02.323624Z","shell.execute_reply":"2024-06-01T18:48:02.322386Z","shell.execute_reply.started":"2024-06-01T18:48:02.312516Z"},"trusted":true},"outputs":[],"source":["def prepare_dataset(dataset_opt):\n","    print(dataset_opt)\n","    # Load dataset from CSV file\n","    df = pd.read_csv(dataset_opt['path'])\n","    dataset = Dataset.from_pandas(df)\n","\n","    # Tokenize text and keyword, and concatenate them\n","    def tokenize_data(example):\n","        text = example[\"cleaned_text\"]\n","        keyword = example[\"cleaned_keyword\"]   \n","        obj = tokenizer(text, keyword, truncation=True, padding=\"max_length\", max_length=168)\n","        obj[\"extra_data\"] = []\n","        if dataset_opt['feature'] == True:\n","            obj[\"extra_data\"] = [\n","                example['hashtag_count'],\n","                example['url_count'],\n","                example['positive'],\n","                example['neutral'],\n","                example['negative'],\n","                example['ner_count']\n","            ]\n","        obj[\"labels\"] = example[\"target\"]\n","        return obj\n","    \n","    # Apply tokenization to all examples in the dataset\n","    dataset = dataset.map(tokenize_data, remove_columns=dataset.column_names)\n","    dataset = dataset.train_test_split(test_size=0.3, shuffle=True)\n","    return dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:48:05.404019Z","iopub.status.busy":"2024-06-01T18:48:05.403307Z","iopub.status.idle":"2024-06-01T18:48:45.081762Z","shell.execute_reply":"2024-06-01T18:48:45.080830Z","shell.execute_reply.started":"2024-06-01T18:48:05.403987Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'path': '../input/disaster-tweets-hlt/preprocess_one.csv', 'feature': False}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0e1e552b6f847d89de447a30d02a892","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'path': '../input/disaster-tweets-hlt/preprocess_two.csv', 'feature': False}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64d5949bb5c544099a4efde4f8132952","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'path': '../input/disaster-tweets-hlt/preprocess_three.csv', 'feature': True}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ea8c2ccbfb84350bfc1de32c05cdd4e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'path': '../input/disaster-tweets-hlt/preprocess_four.csv', 'feature': True}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0784ae0694864306beedd2b73728dafc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["datasets = []\n","for dataset_opt in dataset_opts:\n","    datasets.append(prepare_dataset(dataset_opt))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:48:47.224966Z","iopub.status.busy":"2024-06-01T18:48:47.224217Z","iopub.status.idle":"2024-06-01T18:48:47.229897Z","shell.execute_reply":"2024-06-01T18:48:47.228915Z","shell.execute_reply.started":"2024-06-01T18:48:47.224925Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 5329\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 2284\n","    })\n","}), DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 5329\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 2284\n","    })\n","}), DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 5329\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 2284\n","    })\n","}), DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 5329\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'extra_data', 'labels'],\n","        num_rows: 2284\n","    })\n","})]\n"]}],"source":["print(datasets)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:48:51.586210Z","iopub.status.busy":"2024-06-01T18:48:51.585759Z","iopub.status.idle":"2024-06-01T18:48:51.599837Z","shell.execute_reply":"2024-06-01T18:48:51.598628Z","shell.execute_reply.started":"2024-06-01T18:48:51.586174Z"},"trusted":true},"outputs":[],"source":["# Define evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = logits.argmax(axis=-1)\n","    accuracy = accuracy_score(labels, predictions)\n","    precision = precision_score(labels, predictions)\n","    recall = recall_score(labels, predictions)\n","    f1 = f1_score(labels, predictions)\n","    return { \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1 }\n","\n","def objective(config):\n","    n_features = 0\n","    if dataset_opts[config.dataset - 1]['feature']:\n","        n_features = 6\n","    model = BERTModel.from_pretrained(\n","        model_id, \n","        num_labels=2,\n","        num_extra_dims=n_features,\n","        dropout=config.dp,\n","        n_layers=config.num_layers\n","    )\n","    # Define training arguments\n","    training_args = TrainingArguments(\n","        num_train_epochs=config.epochs,\n","        per_device_train_batch_size=config.batch_size,\n","        per_device_eval_batch_size=config.batch_size,\n","        weight_decay=config.w_decay,\n","        load_best_model_at_end=True,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        logging_dir=\"./logs\",\n","        output_dir=\"./outs\",\n","        report_to=\"wandb\",\n","        logging_steps=1,\n","    )\n","    \n","    optimizer = AdamW(model.parameters(), lr=config.lr)\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=datasets[config.dataset - 1]['train'],\n","        eval_dataset=datasets[config.dataset - 1]['test'],\n","        compute_metrics=compute_metrics,\n","        optimizers=(optimizer, None),\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n","    )\n","\n","    # Search hyperparameters\n","    trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T18:48:56.556089Z","iopub.status.busy":"2024-06-01T18:48:56.555696Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: mp4y5rxk\n","Sweep URL: https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47v8iax3 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdp: 0.54119585658337\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 3.827180383022126e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 12\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tw_decay: 0.006761925303594378\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240601_184858-47v8iax3</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/47v8iax3' target=\"_blank\">gallant-sweep-1</a></strong> to <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/47v8iax3' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/47v8iax3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of CustomDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'distilbert.transformer.layer.10.attention.k_lin.bias', 'distilbert.transformer.layer.10.attention.k_lin.weight', 'distilbert.transformer.layer.10.attention.out_lin.bias', 'distilbert.transformer.layer.10.attention.out_lin.weight', 'distilbert.transformer.layer.10.attention.q_lin.bias', 'distilbert.transformer.layer.10.attention.q_lin.weight', 'distilbert.transformer.layer.10.attention.v_lin.bias', 'distilbert.transformer.layer.10.attention.v_lin.weight', 'distilbert.transformer.layer.10.ffn.lin1.bias', 'distilbert.transformer.layer.10.ffn.lin1.weight', 'distilbert.transformer.layer.10.ffn.lin2.bias', 'distilbert.transformer.layer.10.ffn.lin2.weight', 'distilbert.transformer.layer.10.output_layer_norm.bias', 'distilbert.transformer.layer.10.output_layer_norm.weight', 'distilbert.transformer.layer.10.sa_layer_norm.bias', 'distilbert.transformer.layer.10.sa_layer_norm.weight', 'distilbert.transformer.layer.11.attention.k_lin.bias', 'distilbert.transformer.layer.11.attention.k_lin.weight', 'distilbert.transformer.layer.11.attention.out_lin.bias', 'distilbert.transformer.layer.11.attention.out_lin.weight', 'distilbert.transformer.layer.11.attention.q_lin.bias', 'distilbert.transformer.layer.11.attention.q_lin.weight', 'distilbert.transformer.layer.11.attention.v_lin.bias', 'distilbert.transformer.layer.11.attention.v_lin.weight', 'distilbert.transformer.layer.11.ffn.lin1.bias', 'distilbert.transformer.layer.11.ffn.lin1.weight', 'distilbert.transformer.layer.11.ffn.lin2.bias', 'distilbert.transformer.layer.11.ffn.lin2.weight', 'distilbert.transformer.layer.11.output_layer_norm.bias', 'distilbert.transformer.layer.11.output_layer_norm.weight', 'distilbert.transformer.layer.11.sa_layer_norm.bias', 'distilbert.transformer.layer.11.sa_layer_norm.weight', 'distilbert.transformer.layer.6.attention.k_lin.bias', 'distilbert.transformer.layer.6.attention.k_lin.weight', 'distilbert.transformer.layer.6.attention.out_lin.bias', 'distilbert.transformer.layer.6.attention.out_lin.weight', 'distilbert.transformer.layer.6.attention.q_lin.bias', 'distilbert.transformer.layer.6.attention.q_lin.weight', 'distilbert.transformer.layer.6.attention.v_lin.bias', 'distilbert.transformer.layer.6.attention.v_lin.weight', 'distilbert.transformer.layer.6.ffn.lin1.bias', 'distilbert.transformer.layer.6.ffn.lin1.weight', 'distilbert.transformer.layer.6.ffn.lin2.bias', 'distilbert.transformer.layer.6.ffn.lin2.weight', 'distilbert.transformer.layer.6.output_layer_norm.bias', 'distilbert.transformer.layer.6.output_layer_norm.weight', 'distilbert.transformer.layer.6.sa_layer_norm.bias', 'distilbert.transformer.layer.6.sa_layer_norm.weight', 'distilbert.transformer.layer.7.attention.k_lin.bias', 'distilbert.transformer.layer.7.attention.k_lin.weight', 'distilbert.transformer.layer.7.attention.out_lin.bias', 'distilbert.transformer.layer.7.attention.out_lin.weight', 'distilbert.transformer.layer.7.attention.q_lin.bias', 'distilbert.transformer.layer.7.attention.q_lin.weight', 'distilbert.transformer.layer.7.attention.v_lin.bias', 'distilbert.transformer.layer.7.attention.v_lin.weight', 'distilbert.transformer.layer.7.ffn.lin1.bias', 'distilbert.transformer.layer.7.ffn.lin1.weight', 'distilbert.transformer.layer.7.ffn.lin2.bias', 'distilbert.transformer.layer.7.ffn.lin2.weight', 'distilbert.transformer.layer.7.output_layer_norm.bias', 'distilbert.transformer.layer.7.output_layer_norm.weight', 'distilbert.transformer.layer.7.sa_layer_norm.bias', 'distilbert.transformer.layer.7.sa_layer_norm.weight', 'distilbert.transformer.layer.8.attention.k_lin.bias', 'distilbert.transformer.layer.8.attention.k_lin.weight', 'distilbert.transformer.layer.8.attention.out_lin.bias', 'distilbert.transformer.layer.8.attention.out_lin.weight', 'distilbert.transformer.layer.8.attention.q_lin.bias', 'distilbert.transformer.layer.8.attention.q_lin.weight', 'distilbert.transformer.layer.8.attention.v_lin.bias', 'distilbert.transformer.layer.8.attention.v_lin.weight', 'distilbert.transformer.layer.8.ffn.lin1.bias', 'distilbert.transformer.layer.8.ffn.lin1.weight', 'distilbert.transformer.layer.8.ffn.lin2.bias', 'distilbert.transformer.layer.8.ffn.lin2.weight', 'distilbert.transformer.layer.8.output_layer_norm.bias', 'distilbert.transformer.layer.8.output_layer_norm.weight', 'distilbert.transformer.layer.8.sa_layer_norm.bias', 'distilbert.transformer.layer.8.sa_layer_norm.weight', 'distilbert.transformer.layer.9.attention.k_lin.bias', 'distilbert.transformer.layer.9.attention.k_lin.weight', 'distilbert.transformer.layer.9.attention.out_lin.bias', 'distilbert.transformer.layer.9.attention.out_lin.weight', 'distilbert.transformer.layer.9.attention.q_lin.bias', 'distilbert.transformer.layer.9.attention.q_lin.weight', 'distilbert.transformer.layer.9.attention.v_lin.bias', 'distilbert.transformer.layer.9.attention.v_lin.weight', 'distilbert.transformer.layer.9.ffn.lin1.bias', 'distilbert.transformer.layer.9.ffn.lin1.weight', 'distilbert.transformer.layer.9.ffn.lin2.bias', 'distilbert.transformer.layer.9.ffn.lin2.weight', 'distilbert.transformer.layer.9.output_layer_norm.bias', 'distilbert.transformer.layer.9.output_layer_norm.weight', 'distilbert.transformer.layer.9.sa_layer_norm.bias', 'distilbert.transformer.layer.9.sa_layer_norm.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='294' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [294/420 11:27 < 04:56, 0.42 it/s, Epoch 7/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.456300</td>\n","      <td>0.520718</td>\n","      <td>0.792032</td>\n","      <td>0.840052</td>\n","      <td>0.647295</td>\n","      <td>0.731183</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.315900</td>\n","      <td>0.482181</td>\n","      <td>0.810858</td>\n","      <td>0.882432</td>\n","      <td>0.654309</td>\n","      <td>0.751438</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.585500</td>\n","      <td>0.468656</td>\n","      <td>0.794658</td>\n","      <td>0.767984</td>\n","      <td>0.759519</td>\n","      <td>0.763728</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.372300</td>\n","      <td>0.499111</td>\n","      <td>0.783713</td>\n","      <td>0.732902</td>\n","      <td>0.794589</td>\n","      <td>0.762500</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.327000</td>\n","      <td>0.489334</td>\n","      <td>0.802102</td>\n","      <td>0.784375</td>\n","      <td>0.754509</td>\n","      <td>0.769152</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.462900</td>\n","      <td>0.481810</td>\n","      <td>0.817426</td>\n","      <td>0.827508</td>\n","      <td>0.735471</td>\n","      <td>0.778780</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.384500</td>\n","      <td>0.491454</td>\n","      <td>0.816112</td>\n","      <td>0.836047</td>\n","      <td>0.720441</td>\n","      <td>0.773950</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-42)... Done. 4.5s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-84)... Done. 4.4s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-126)... Done. 4.2s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-168)... Done. 4.2s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-210)... Done. 4.4s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-252)... Done. 4.3s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-294)... Done. 4.4s\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='9240.106 MB of 9240.106 MB uploaded (0.031 MB deduped)\\r'), FloatProgress(value=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▃▇▃▁▅██</td></tr><tr><td>eval/f1</td><td>▁▄▆▆▇█▇</td></tr><tr><td>eval/loss</td><td>█▃▁▅▄▃▄</td></tr><tr><td>eval/precision</td><td>▆█▃▁▃▅▆</td></tr><tr><td>eval/recall</td><td>▁▁▆█▆▅▄</td></tr><tr><td>eval/runtime</td><td>▁▇█▇▇▇▇</td></tr><tr><td>eval/samples_per_second</td><td>█▂▁▂▂▂▁</td></tr><tr><td>eval/steps_per_second</td><td>█▂▁▂▁▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▅▄▄▅▂▅▆▄▃▆▄▄▅▃▆▃▂▅▄▃▂▆▅▂▂▂█▂▃▂▂▅▃▆▄▁▄▂▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▅▄▄▃▃▄▄▃▆▁▄▆▄▄▄▃▄▂▃▃▅▃▃▃▂▃▃▃▃▃▄▁▄▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.81611</td></tr><tr><td>eval/f1</td><td>0.77395</td></tr><tr><td>eval/loss</td><td>0.49145</td></tr><tr><td>eval/precision</td><td>0.83605</td></tr><tr><td>eval/recall</td><td>0.72044</td></tr><tr><td>eval/runtime</td><td>13.2953</td></tr><tr><td>eval/samples_per_second</td><td>171.79</td></tr><tr><td>eval/steps_per_second</td><td>1.354</td></tr><tr><td>total_flos</td><td>3243047316340608.0</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>294</td></tr><tr><td>train/grad_norm</td><td>5.39141</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.3845</td></tr><tr><td>train_loss</td><td>0.4514</td></tr><tr><td>train_runtime</td><td>680.1737</td></tr><tr><td>train_samples_per_second</td><td>78.348</td></tr><tr><td>train_steps_per_second</td><td>0.617</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">gallant-sweep-1</strong> at: <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/47v8iax3' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/47v8iax3</a><br/> View project at: <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 52 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240601_184858-47v8iax3/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: guusfwkz with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdp: 0.7395366608255726\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 8.280365700118994e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 14\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tw_decay: 0.0014348933025313268\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240601_190104-guusfwkz</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/guusfwkz' target=\"_blank\">azure-sweep-2</a></strong> to <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/sweeps/mp4y5rxk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/guusfwkz' target=\"_blank\">https://wandb.ai/hlt/DISTILBERT-Finetuning/runs/guusfwkz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of CustomDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'distilbert.transformer.layer.10.attention.k_lin.bias', 'distilbert.transformer.layer.10.attention.k_lin.weight', 'distilbert.transformer.layer.10.attention.out_lin.bias', 'distilbert.transformer.layer.10.attention.out_lin.weight', 'distilbert.transformer.layer.10.attention.q_lin.bias', 'distilbert.transformer.layer.10.attention.q_lin.weight', 'distilbert.transformer.layer.10.attention.v_lin.bias', 'distilbert.transformer.layer.10.attention.v_lin.weight', 'distilbert.transformer.layer.10.ffn.lin1.bias', 'distilbert.transformer.layer.10.ffn.lin1.weight', 'distilbert.transformer.layer.10.ffn.lin2.bias', 'distilbert.transformer.layer.10.ffn.lin2.weight', 'distilbert.transformer.layer.10.output_layer_norm.bias', 'distilbert.transformer.layer.10.output_layer_norm.weight', 'distilbert.transformer.layer.10.sa_layer_norm.bias', 'distilbert.transformer.layer.10.sa_layer_norm.weight', 'distilbert.transformer.layer.11.attention.k_lin.bias', 'distilbert.transformer.layer.11.attention.k_lin.weight', 'distilbert.transformer.layer.11.attention.out_lin.bias', 'distilbert.transformer.layer.11.attention.out_lin.weight', 'distilbert.transformer.layer.11.attention.q_lin.bias', 'distilbert.transformer.layer.11.attention.q_lin.weight', 'distilbert.transformer.layer.11.attention.v_lin.bias', 'distilbert.transformer.layer.11.attention.v_lin.weight', 'distilbert.transformer.layer.11.ffn.lin1.bias', 'distilbert.transformer.layer.11.ffn.lin1.weight', 'distilbert.transformer.layer.11.ffn.lin2.bias', 'distilbert.transformer.layer.11.ffn.lin2.weight', 'distilbert.transformer.layer.11.output_layer_norm.bias', 'distilbert.transformer.layer.11.output_layer_norm.weight', 'distilbert.transformer.layer.11.sa_layer_norm.bias', 'distilbert.transformer.layer.11.sa_layer_norm.weight', 'distilbert.transformer.layer.12.attention.k_lin.bias', 'distilbert.transformer.layer.12.attention.k_lin.weight', 'distilbert.transformer.layer.12.attention.out_lin.bias', 'distilbert.transformer.layer.12.attention.out_lin.weight', 'distilbert.transformer.layer.12.attention.q_lin.bias', 'distilbert.transformer.layer.12.attention.q_lin.weight', 'distilbert.transformer.layer.12.attention.v_lin.bias', 'distilbert.transformer.layer.12.attention.v_lin.weight', 'distilbert.transformer.layer.12.ffn.lin1.bias', 'distilbert.transformer.layer.12.ffn.lin1.weight', 'distilbert.transformer.layer.12.ffn.lin2.bias', 'distilbert.transformer.layer.12.ffn.lin2.weight', 'distilbert.transformer.layer.12.output_layer_norm.bias', 'distilbert.transformer.layer.12.output_layer_norm.weight', 'distilbert.transformer.layer.12.sa_layer_norm.bias', 'distilbert.transformer.layer.12.sa_layer_norm.weight', 'distilbert.transformer.layer.13.attention.k_lin.bias', 'distilbert.transformer.layer.13.attention.k_lin.weight', 'distilbert.transformer.layer.13.attention.out_lin.bias', 'distilbert.transformer.layer.13.attention.out_lin.weight', 'distilbert.transformer.layer.13.attention.q_lin.bias', 'distilbert.transformer.layer.13.attention.q_lin.weight', 'distilbert.transformer.layer.13.attention.v_lin.bias', 'distilbert.transformer.layer.13.attention.v_lin.weight', 'distilbert.transformer.layer.13.ffn.lin1.bias', 'distilbert.transformer.layer.13.ffn.lin1.weight', 'distilbert.transformer.layer.13.ffn.lin2.bias', 'distilbert.transformer.layer.13.ffn.lin2.weight', 'distilbert.transformer.layer.13.output_layer_norm.bias', 'distilbert.transformer.layer.13.output_layer_norm.weight', 'distilbert.transformer.layer.13.sa_layer_norm.bias', 'distilbert.transformer.layer.13.sa_layer_norm.weight', 'distilbert.transformer.layer.6.attention.k_lin.bias', 'distilbert.transformer.layer.6.attention.k_lin.weight', 'distilbert.transformer.layer.6.attention.out_lin.bias', 'distilbert.transformer.layer.6.attention.out_lin.weight', 'distilbert.transformer.layer.6.attention.q_lin.bias', 'distilbert.transformer.layer.6.attention.q_lin.weight', 'distilbert.transformer.layer.6.attention.v_lin.bias', 'distilbert.transformer.layer.6.attention.v_lin.weight', 'distilbert.transformer.layer.6.ffn.lin1.bias', 'distilbert.transformer.layer.6.ffn.lin1.weight', 'distilbert.transformer.layer.6.ffn.lin2.bias', 'distilbert.transformer.layer.6.ffn.lin2.weight', 'distilbert.transformer.layer.6.output_layer_norm.bias', 'distilbert.transformer.layer.6.output_layer_norm.weight', 'distilbert.transformer.layer.6.sa_layer_norm.bias', 'distilbert.transformer.layer.6.sa_layer_norm.weight', 'distilbert.transformer.layer.7.attention.k_lin.bias', 'distilbert.transformer.layer.7.attention.k_lin.weight', 'distilbert.transformer.layer.7.attention.out_lin.bias', 'distilbert.transformer.layer.7.attention.out_lin.weight', 'distilbert.transformer.layer.7.attention.q_lin.bias', 'distilbert.transformer.layer.7.attention.q_lin.weight', 'distilbert.transformer.layer.7.attention.v_lin.bias', 'distilbert.transformer.layer.7.attention.v_lin.weight', 'distilbert.transformer.layer.7.ffn.lin1.bias', 'distilbert.transformer.layer.7.ffn.lin1.weight', 'distilbert.transformer.layer.7.ffn.lin2.bias', 'distilbert.transformer.layer.7.ffn.lin2.weight', 'distilbert.transformer.layer.7.output_layer_norm.bias', 'distilbert.transformer.layer.7.output_layer_norm.weight', 'distilbert.transformer.layer.7.sa_layer_norm.bias', 'distilbert.transformer.layer.7.sa_layer_norm.weight', 'distilbert.transformer.layer.8.attention.k_lin.bias', 'distilbert.transformer.layer.8.attention.k_lin.weight', 'distilbert.transformer.layer.8.attention.out_lin.bias', 'distilbert.transformer.layer.8.attention.out_lin.weight', 'distilbert.transformer.layer.8.attention.q_lin.bias', 'distilbert.transformer.layer.8.attention.q_lin.weight', 'distilbert.transformer.layer.8.attention.v_lin.bias', 'distilbert.transformer.layer.8.attention.v_lin.weight', 'distilbert.transformer.layer.8.ffn.lin1.bias', 'distilbert.transformer.layer.8.ffn.lin1.weight', 'distilbert.transformer.layer.8.ffn.lin2.bias', 'distilbert.transformer.layer.8.ffn.lin2.weight', 'distilbert.transformer.layer.8.output_layer_norm.bias', 'distilbert.transformer.layer.8.output_layer_norm.weight', 'distilbert.transformer.layer.8.sa_layer_norm.bias', 'distilbert.transformer.layer.8.sa_layer_norm.weight', 'distilbert.transformer.layer.9.attention.k_lin.bias', 'distilbert.transformer.layer.9.attention.k_lin.weight', 'distilbert.transformer.layer.9.attention.out_lin.bias', 'distilbert.transformer.layer.9.attention.out_lin.weight', 'distilbert.transformer.layer.9.attention.q_lin.bias', 'distilbert.transformer.layer.9.attention.q_lin.weight', 'distilbert.transformer.layer.9.attention.v_lin.bias', 'distilbert.transformer.layer.9.attention.v_lin.weight', 'distilbert.transformer.layer.9.ffn.lin1.bias', 'distilbert.transformer.layer.9.ffn.lin1.weight', 'distilbert.transformer.layer.9.ffn.lin2.bias', 'distilbert.transformer.layer.9.ffn.lin2.weight', 'distilbert.transformer.layer.9.output_layer_norm.bias', 'distilbert.transformer.layer.9.output_layer_norm.weight', 'distilbert.transformer.layer.9.sa_layer_norm.bias', 'distilbert.transformer.layer.9.sa_layer_norm.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='163' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [163/420 07:02 < 11:13, 0.38 it/s, Epoch 3.86/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.781000</td>\n","      <td>0.687759</td>\n","      <td>0.563047</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.809200</td>\n","      <td>0.681082</td>\n","      <td>0.563047</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.704600</td>\n","      <td>0.688343</td>\n","      <td>0.563047</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-42)... Done. 5.0s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-84)... Done. 5.0s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outs/checkpoint-126)... Done. 5.0s\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["def main():\n","    wandb.init()\n","    objective(wandb.config)\n","\n","sweep_id = wandb.sweep(sweep=sweep_configuration, project=os.environ[\"WANDB_PROJECT\"])\n","wandb.agent(sweep_id, function=main, count=10)"]},{"cell_type":"markdown","metadata":{},"source":["# MAMBA FINETUNING"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q git+https://github.com/huggingface/transformers@main\n","!pip install -q causal-conv1d>=1.2.0 mamba-ssm trl peft datasets\n","!pip install accelerate -U -q\n","!git clone https://github.com/getorca/mamba_for_sequence_classification.git\n","!pip install ./mamba_for_sequence_classification/. -q\n","!pip install bnb -q"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer\n","from hf_mamba_classification import MambaForSequenceClassification\n","from datasets import Dataset\n","import pandas as pd\n","import torch\n","import wandb\n","import os\n","\n","wandb.login(key=\"b52c0b8bafc7f2f71a0cbd30c1b2d736a881787f\")\n","os.environ[\"WANDB_PROJECT\"] = \"MAMBA-Finetuning\"  # name your W&B project\n","os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\n","model_id = \"state-spaces/mamba-130m-hf\"\n","id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.set_device(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset_opts = [\n","    {\n","        'path': '../data-to-train/preprocess_one.csv',\n","        'feature': False,\n","    },\n","        {\n","        'path': '../data-to-train/preprocess_two.csv',\n","        'feature': False,\n","    },\n","        {\n","        'path': '../data-to-train/preprocess_three.csv',\n","        'feature': True,\n","    },\n","        {\n","        'path': '../data-to-train/preprocess_four.csv',\n","        'feature': True,\n","    },\n","]\n","# Sweep configuration\n","sweep_configuration = {\n","    \"method\": \"bayes\",\n","    \"name\": \"sweep\",\n","    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval/f1\"},\n","    \"parameters\": {\n","        \"batch_size\": {\"values\": [8, 16, 32, 64]},\n","        \"epochs\": {\"values\": [10]},\n","        \"lr\": {\"max\": 1e-4, \"min\": 5e-5},\n","        \"dataset\": {\"values\": [1, 2, 3, 4]},\n","    },\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_dataset(dataset_opt):\n","    # Load dataset from CSV file\n","    df = pd.read_csv(dataset_opt['path'])\n","    dataset = Dataset.from_pandas(df)\n","\n","    # Tokenize text and keyword, and concatenate them\n","    def tokenize_data(example):\n","        text = example[\"cleaned_text\"]\n","        keyword = example[\"cleaned_keyword\"]\n","        if dataset_opt['feature']:\n","            keyword += \",\" + str(example['hashtag_count'])\n","            keyword += \",\" + str(example['url_count'])\n","            keyword += \",\" + str(example['positive'])\n","            keyword += \",\" + str(example['neutral'])\n","            keyword += \",\" + str(example['negative'])\n","            keyword += \",\" + str(example['ner_count'])\n","        obj = tokenizer(text, keyword, truncation=True, padding=\"max_length\", max_length=256)\n","        obj[\"labels\"] = example[\"target\"]\n","        return obj\n","    \n","    # Apply tokenization to all examples in the dataset\n","    dataset = dataset.map(tokenize_data, remove_columns=dataset.column_names)\n","    dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datasets = []\n","for dataset_opt in dataset_opts:\n","    datasets.append(prepare_dataset(dataset_opt))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = logits.argmax(axis=-1)\n","    accuracy = accuracy_score(labels, predictions)\n","    precision = precision_score(labels, predictions)\n","    recall = recall_score(labels, predictions)\n","    f1 = f1_score(labels, predictions)\n","    return { \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1 }\n","\n","def objective(config):\n","    model = MambaForSequenceClassification.from_pretrained(\n","        model_id, \n","        num_labels=2, \n","        id2label=id2label, \n","        label2id=label2id,\n","        use_cache=False  # This needs to be passed when using eval and training Mamba for sequence classification otherwise it will raise an error\n","    )\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"mamba_imdb_classification\",\n","        learning_rate=config.lr,\n","        per_device_train_batch_size=config.batch_size,\n","        per_device_eval_batch_size=config.batch_size,\n","        num_train_epochs=config.epochs,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        # eval_steps=1000,\n","        save_strategy=\"epoch\",\n","        # load_best_model_at_end=True,\n","        lr_scheduler_type=\"cosine\",\n","        # optim='paged_adamw_8bit',\n","        # push_to_hub=True,  \n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=datasets[config.dataset - 1]['train'],\n","        eval_dataset=datasets[config.dataset - 1]['test'],\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def main():\n","    wandb.init()\n","    objective(wandb.config)\n","\n","sweep_id = wandb.sweep(sweep=sweep_configuration, project=os.environ[\"WANDB_PROJECT\"])\n","\n","wandb.agent(sweep_id, function=main, count=10)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5040447,"sourceId":8456876,"sourceType":"datasetVersion"},{"datasetId":5044223,"sourceId":8461741,"sourceType":"datasetVersion"},{"datasetId":5110303,"sourceId":8551725,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
