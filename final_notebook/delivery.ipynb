{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS\n",
    "In this section we gonna explore the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['target']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each target value\n",
    "target_counts = df['target'].value_counts()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "target_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Target Values')\n",
    "plt.xlabel('Target Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels if needed\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keyword'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count NaN values in 'location' and 'keyword' columns\n",
    "nan_location_count = df['location'].isna().sum()\n",
    "nan_keyword_count = df['keyword'].isna().sum()\n",
    "\n",
    "# Count non-NaN values in 'location' and 'keyword' columns\n",
    "non_nan_location_count = df['location'].notna().sum()\n",
    "non_nan_keyword_count = df['keyword'].notna().sum()\n",
    "\n",
    "# Create labels for the pie charts\n",
    "labels_location = [f'NaN \\n ({nan_location_count})', f'Non-NaN \\n ({non_nan_location_count})']\n",
    "labels_keyword = [f'NaN \\n ({nan_keyword_count})', f'Non-NaN \\n ({non_nan_keyword_count})']\n",
    "\n",
    "# Create data for the pie charts\n",
    "sizes_location = [nan_location_count, non_nan_location_count]\n",
    "sizes_keyword = [nan_keyword_count, non_nan_keyword_count]\n",
    "\n",
    "# Create colors for different sections of the pie charts\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "\n",
    "# Plot the pie chart for 'location' column\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sizes_location, labels=labels_location, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Proportions of NaN values in Location')\n",
    "\n",
    "\n",
    "# Plot the pie chart for 'keyword' column\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(sizes_keyword, labels=labels_keyword, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Proportions of NaN Values in Keyword')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the pie charts\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique locations with target equal to 1\n",
    "unique_locations_target_one = df[df['target'] == 1]['location'].nunique()\n",
    "\n",
    "# Get unique locations with target equal to 0\n",
    "unique_locations_target_zero = df[df['target'] == 0]['location'].nunique()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Target=1', 'Target=0'], [unique_locations_target_one, unique_locations_target_zero], color=['#ff9999', '#66b3ff'])\n",
    "plt.title('Number of Unique Locations with Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Number of Unique Locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique locations with target equal to 1\n",
    "unique_keywords_target_one = df[df['target'] == 1]['keyword'].nunique()\n",
    "\n",
    "# Get unique locations with target equal to 0\n",
    "unique_keywords_target_zero = df[df['target'] == 0]['keyword'].nunique()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Target=1', 'Target=0'], [unique_keywords_target_one, unique_keywords_target_zero], color=['#ff9999', '#66b3ff'])\n",
    "plt.title('Number of Unique Keywords with Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Number of Unique Keywords')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each text and create a new column 'text_length'\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "\n",
    "# Calculate the average and maximum text length\n",
    "average_text_length = df['text_length'].mean()\n",
    "max_text_length = df['text_length'].max()\n",
    "\n",
    "print(\"Average text length:\", average_text_length)\n",
    "print(\"Maximum text length:\", max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_text_len_targer = df.groupby('target')['text_length'].mean()\n",
    "\n",
    "print(average_text_len_targer)\n",
    "\n",
    "# Plot the average word count in a histogram\n",
    "average_text_len_targer.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Text Length by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Text Lenght')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='text_length', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth=10)\n",
    "plt.title('Distribution of Text Lenght Counts by Target')\n",
    "plt.xlabel('Text Lenght')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate word count\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['word_count'] = df['text'].apply(lambda x: word_count(x))\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average word count by target\n",
    "average_word_count = df.groupby('target')['word_count'].mean()\n",
    "print(average_word_count)\n",
    "\n",
    "# Plot the average word count in a histogram\n",
    "average_word_count.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Word Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='word_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth=3)\n",
    "plt.title('Distribution of WordCounts by Target')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stop words data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to calculate stop word ratio\n",
    "def stop_word_ratio(text):\n",
    "    words = text.split()\n",
    "    num_stop_words = sum(1 for word in words if word.lower() in stop_words)\n",
    "    return num_stop_words / len(words) if len(words) > 0 else 0\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['stop_word_ratio'] = df['text'].apply(stop_word_ratio)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average stop word ratio by target\n",
    "average_stop_word_ratio = df.groupby('target')['stop_word_ratio'].mean()\n",
    "print(average_stop_word_ratio)\n",
    "\n",
    "average_stop_word_ratio.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Stop Word Ratio by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Stop Word Ratio')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='stop_word_ratio', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.title('Distribution of Stop Words Ratio by Target')\n",
    "plt.xlabel('Stops Word Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate unique word count\n",
    "def unique_word_count(text):\n",
    "    words = word_tokenize(text.lower())  # Tokenize text and convert to lowercase\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words)\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['unique_word_count'] = df['text'].apply(unique_word_count)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average unique word count by target\n",
    "average_unique_word_count = df.groupby('target')['unique_word_count'].mean()\n",
    "\n",
    "# Display the average unique word count\n",
    "print(average_unique_word_count)\n",
    "\n",
    "average_unique_word_count.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Unique Word Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Unique Word Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='unique_word_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth=3)\n",
    "plt.title('Distribution of Unique Words Count by Target')\n",
    "plt.xlabel('Unique Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Type-Token Ratio (TTR)\n",
    "def ttr(text):\n",
    "    words = word_tokenize(text.lower())  # Tokenize text and convert to lowercase\n",
    "    total_words = len(words)\n",
    "    unique_words = set(words)\n",
    "    total_unique_words = len(unique_words)\n",
    "    if total_words > 0:\n",
    "        return total_unique_words / total_words\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['ttr'] = df['text'].apply(ttr)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ttr = df.groupby('target')['ttr'].mean()\n",
    "\n",
    "# Display the average ttr \n",
    "print(average_ttr)\n",
    "\n",
    "average_ttr.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average TTR by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average TTR')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='ttr', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.title('Distribution of TTR by Target')\n",
    "plt.xlabel('TTR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Shannon entropy\n",
    "def entropy(text):\n",
    "    # Count the occurrences of each character in the text\n",
    "    counts = Counter(text.lower())\n",
    "    # Total number of characters\n",
    "    total_chars = sum(counts.values())\n",
    "    # Calculate the probability of each character\n",
    "    probs = [count / total_chars for count in counts.values()]\n",
    "    # Calculate Shannon entropy\n",
    "    entropy = -np.sum([prob * np.log2(prob) for prob in probs])\n",
    "    return entropy\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['entropy'] = df['text'].apply(entropy)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_entropy = df.groupby('target')['entropy'].mean()\n",
    "\n",
    "print(average_entropy)\n",
    "\n",
    "average_entropy.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Entropy by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Entropy')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='entropy', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.title('Distribution of Entropy by Target')\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to count punctuation symbols\n",
    "def punctuation_count(text):\n",
    "    # Define punctuation characters\n",
    "    punctuation_chars = set(string.punctuation)\n",
    "    # Count occurrences of punctuation symbols in the text\n",
    "    punctuation_count = sum(1 for char in text if char in punctuation_chars)\n",
    "    return punctuation_count\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['punctuation_count'] = df['text'].apply(punctuation_count)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_punctuation = df.groupby('target')['punctuation_count'].mean()\n",
    "\n",
    "print(average_punctuation)\n",
    "\n",
    "average_punctuation.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Punctation Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Punctuation Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(19, 10))\n",
    "sns.displot(data= df, x='punctuation_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.xlim(0, 30)\n",
    "plt.title('Distribution of Punctuation Count by Target')\n",
    "plt.xlabel('Punctuation Count ')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count named entities\n",
    "def ner_count(text):\n",
    "    doc = nlp(text)\n",
    "    # Count unique named entities\n",
    "    entities = set([ent.label_ for ent in doc.ents])\n",
    "    return len(entities)\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['ner_count'] = df['text'].apply(ner_count)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ner = df.groupby('target')['ner_count'].mean()\n",
    "\n",
    "print(average_ner)\n",
    "\n",
    "average_ner.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average NER Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average NER Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='ner_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], discrete = True)\n",
    "plt.xlim(0, 5)\n",
    "plt.title('Distribution of NER Count by Target')\n",
    "plt.xlabel('NER Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count POS tags\n",
    "def pos_count(text, pos_tags):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_tags:\n",
    "            pos_counts += 1\n",
    "    return pos_counts\n",
    "\n",
    "# Define the POS tags you want to count\n",
    "pos_tags = ['NOUN', 'ADJ', 'VERB', 'ADP', 'PRON']\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['tot_pos_counts'] = df['text'].apply(lambda x: pos_count(x, pos_tags))\n",
    "df['NOUN_counts'] = df['text'].apply(lambda x: pos_count(x, ['NOUN']))\n",
    "df['ADJ_counts'] = df['text'].apply(lambda x: pos_count(x, ['ADJ']))\n",
    "df['VERB_counts'] = df['text'].apply(lambda x: pos_count(x, ['VERB']))\n",
    "df['ADP_counts'] = df['text'].apply(lambda x: pos_count(x, ['ADP']))\n",
    "df['PRON_counts'] = df['text'].apply(lambda x: pos_count(x, ['PRON']))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('analyzed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_tot_pos = df.groupby('target')['tot_pos_counts'].mean()\n",
    "\n",
    "print(average_tot_pos)\n",
    "\n",
    "average_tot_pos.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Total POS Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Total POS Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='tot_pos_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.title('Distribution of Total POS Count by Target')\n",
    "plt.xlabel('POS Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_nouns = df.groupby('target')['NOUN_counts'].mean()\n",
    "\n",
    "print(average_nouns)\n",
    "\n",
    "average_nouns.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average NOUN Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average NOUN Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='NOUN_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n",
    "plt.xlim(0, 15)\n",
    "plt.title('Distribution of NOUN Count by Target')\n",
    "plt.xlabel('NOUN Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_adj = df.groupby('target')['ADJ_counts'].mean()\n",
    "\n",
    "print(average_adj)\n",
    "\n",
    "average_adj.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average ADJ Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average ADJ Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='ADJ_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n",
    "plt.xlim(0, 6)\n",
    "plt.title('Distribution of ADJ Count by Target')\n",
    "plt.xlabel('ADJ Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_verb = df.groupby('target')['VERB_counts'].mean()\n",
    "\n",
    "print(average_verb)\n",
    "\n",
    "average_verb.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average VERB Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average VERB Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='VERB_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n",
    "plt.xlim(0, 7)\n",
    "plt.title('Distribution of VERB Count by Target')\n",
    "plt.xlabel('VERB Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_adp = df.groupby('target')['ADP_counts'].mean()\n",
    "\n",
    "print(average_adp)\n",
    "\n",
    "average_adp.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average ADP Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average ADP Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='ADP_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n",
    "plt.xlim(0, 6)\n",
    "plt.title('Distribution of ADP Count by Target')\n",
    "plt.xlabel('ADP Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_pron = df.groupby('target')['PRON_counts'].mean()\n",
    "\n",
    "print(average_pron)\n",
    "\n",
    "average_pron.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average PRON Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average PRON Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='PRON_counts', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n",
    "plt.xlim(0, 6)\n",
    "plt.title('Distribution of PRON Count by Target')\n",
    "plt.xlabel('PRON Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count subordinate clauses\n",
    "def count_subordinate_clauses(text):\n",
    "    doc = nlp(text)\n",
    "    # Initialize counter for subordinate clauses\n",
    "    subordinate_clause_count = 0\n",
    "    # Iterate over the tokens in the document\n",
    "    for token in doc:\n",
    "        # Check if the token is a subordinating conjunction or a dependent marker\n",
    "        if token.dep_ in ['mark', 'advcl', 'ccomp']:\n",
    "            subordinate_clause_count += 1\n",
    "    return subordinate_clause_count\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['subordinate_clause_count'] = df['text'].apply(count_subordinate_clauses)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_subordinate = df.groupby('target')['subordinate_clause_count'].mean()\n",
    "\n",
    "print(average_subordinate)\n",
    "\n",
    "average_subordinate.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Subordinate Clauses by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Subordinate Clauses')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='subordinate_clause_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.xlim(0, 6)\n",
    "plt.title('Distribution of Subordinate Clause Count by Target')\n",
    "plt.xlabel('Subordinate Clause Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count passive constructions\n",
    "def count_passive_constructions(text):\n",
    "    doc = nlp(text)\n",
    "    # Initialize counter for passive constructions\n",
    "    passive_construction_count = 0\n",
    "    # Iterate over the tokens in the document\n",
    "    for token in doc:\n",
    "        # Check if the token is a verb in passive voice (identified by its dependency relation)\n",
    "        if token.dep_ == 'pass' and token.head.pos_ == 'VERB':\n",
    "            passive_construction_count += 1\n",
    "    return passive_construction_count\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['passive_construction_count'] = df['text'].apply(count_passive_constructions)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_passive = df.groupby('target')['passive_construction_count'].mean()\n",
    "\n",
    "print(average_passive)\n",
    "\n",
    "average_passive.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Passive Constructions by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Passive Constructions')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def get_scores(scores):\n",
    "    score = dict()\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        score[l] = np.round(float(s), 4)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "df[\"positive\"] = 0\n",
    "df[\"neutral\"] = 0\n",
    "df[\"negative\"] = 0\n",
    "\n",
    "\n",
    "for i in tqdm(df.index):\n",
    "    text = df[\"text\"][i]\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    dict_scores = get_scores(scores)\n",
    "    df[\"positive\"][i] = dict_scores[\"positive\"]\n",
    "    df[\"neutral\"][i] = dict_scores[\"neutral\"]\n",
    "    df[\"negative\"][i] = dict_scores[\"negative\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('analyzed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sentiment scores for each target group\n",
    "average_scores = df.groupby('target')[['positive', 'neutral', 'negative',]].mean()\n",
    "\n",
    "# Plot the average sentiment scores\n",
    "average_scores.plot(kind='bar', figsize=(8, 6))\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.title('Average Sentiment Score by Target')\n",
    "plt.xticks([0, 1], ['Target 0', 'Target 1'], rotation=0)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count numbers\n",
    "def count_numbers(text):\n",
    "    numbers = sum(c.isdigit() for c in text)\n",
    "    return numbers\n",
    "\n",
    "# Apply the function to each row of the 'text' column\n",
    "df['number_count'] = df['text'].apply(count_numbers)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average number count by target\n",
    "average_number_count = df.groupby('target')['number_count'].mean()\n",
    "\n",
    "print(average_number_count)\n",
    "\n",
    "# Plot the average number count\n",
    "average_number_count.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Number Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Number Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='number_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 2)\n",
    "plt.xlim(0, 20)\n",
    "plt.title('Distribution of Number Count by Target')\n",
    "plt.xlabel('Number Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    tokens = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "        \n",
    "# Unigrams\n",
    "disaster_unigrams = defaultdict(int)\n",
    "nondisaster_unigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[df['target'] == 1]['text']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        disaster_unigrams[word] += 1\n",
    "        \n",
    "for tweet in df[df['target'] == 0]['text']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        nondisaster_unigrams[word] += 1\n",
    "        \n",
    "df_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "# Bigrams\n",
    "disaster_bigrams = defaultdict(int)\n",
    "nondisaster_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[df['target'] == 1]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        disaster_bigrams[word] += 1\n",
    "        \n",
    "for tweet in df[df['target'] == 0]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        nondisaster_bigrams[word] += 1\n",
    "        \n",
    "df_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "# Trigrams\n",
    "disaster_trigrams = defaultdict(int)\n",
    "nondisaster_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[df['target'] == 1]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        disaster_trigrams[word] += 1\n",
    "        \n",
    "for tweet in df[df['target'] == 0]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        nondisaster_trigrams[word] += 1\n",
    "        \n",
    "df_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='blue')\n",
    "sns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='orange')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='blue')\n",
    "sns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='orange')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(20, 50), dpi=100)\n",
    "\n",
    "sns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='blue')\n",
    "sns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='orange')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=11)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('analyzed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count URLs in a given text\n",
    "def count_urls(text):\n",
    "    url_pattern = r'(https?://)?(www\\.)?\\S+\\.\\S+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return len(urls)\n",
    "\n",
    "# Applying the function to the 'text' column\n",
    "df['url_count'] = df['text'].apply(count_urls)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_urls = df.groupby('target')['url_count'].mean()\n",
    "\n",
    "print(average_urls)\n",
    "\n",
    "average_urls.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average URLs Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average URLs Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='url_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'], binwidth = 1)\n",
    "plt.title('Distribution of URL Count by Target')\n",
    "plt.xlabel('URL Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(text):\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    return len(hashtags)\n",
    "\n",
    "# Applying the function to the 'text' column\n",
    "df['hashtag_count'] = df['text'].apply(count_hashtags)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_hashtags = df.groupby('target')['hashtag_count'].mean()\n",
    "\n",
    "print(average_hashtags)\n",
    "\n",
    "average_hashtags.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Average Hashtags Count by Target')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Average Hashtags Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data= df, x='hashtag_count', hue='target', kde=True, multiple='dodge', palette=['blue', 'orange'])\n",
    "plt.xlim(0, 8)\n",
    "plt.title('Distribution of Hashtag Count by Target')\n",
    "plt.xlabel('Hashtag Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analyzed_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from geopy.geocoders import Nominatim\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from spacy.tokens import Doc\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "source_data_path = '../data/train.csv'\n",
    "processed_data_path = '../cleaned_train.csv'\n",
    "\n",
    "df = pd.read_csv(source_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaCyPreProcessor:\n",
    "\n",
    "    def __init__(self, spacy_model = None, remove_numbers = False, remove_special = False, pos_to_remove = None, remove_stop_words = False, lemmatize = False, use_gpu = False) -> None:\n",
    "        \n",
    "        self.__remove_numbers = remove_numbers\n",
    "        self.__remove_special = remove_special\n",
    "        self.__pos_to_remove = pos_to_remove\n",
    "        self.__remove_stop_words = remove_stop_words\n",
    "        self.__lemmatize = lemmatize\n",
    "\n",
    "        if spacy_model is None:\n",
    "            self.model = spacy.load(\"en_core_web_sm\")\n",
    "        else:\n",
    "            self.model = spacy_model\n",
    "\n",
    "        if use_gpu:\n",
    "            spacy.prefer_gpu()\n",
    "\n",
    "    @staticmethod\n",
    "    def download_spacy_model(model=\"en_core_web_sm\"):\n",
    "        print(f\"Downloading spaCy model {model}\")\n",
    "        spacy.cli.download(model)\n",
    "        print(f\"Finished downloading model\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model=\"en_core_web_sm\"):\n",
    "        return spacy.load(model, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "    def tokenize(self, text) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text using a spaCy pipeline\n",
    "        :param text: Text to tokenize\n",
    "        :return: list of str\n",
    "        \"\"\"\n",
    "        doc = self.model(text)\n",
    "        return [token.text for token in doc]\n",
    "\n",
    "    def preprocess_text(self, text) -> str:\n",
    "        \"\"\"\n",
    "        Runs a spaCy pipeline and removes unwanted parts from text\n",
    "        :param text: text string to clean\n",
    "        :return: str, clean text\n",
    "        \"\"\"\n",
    "        doc = self.model(text)\n",
    "        return self.__clean(doc)\n",
    "\n",
    "    def preprocess_text_list(self, texts=List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Runs a spaCy pipeline and removes unwantes parts from a list of text.\n",
    "        Leverages spaCy's `pipe` for faster batch processing.\n",
    "        :param texts: List of texts to clean\n",
    "        :return: List of clean texts\n",
    "        \"\"\"\n",
    "        clean_texts = []\n",
    "        for doc in tqdm(self.model.pipe(texts)):\n",
    "            clean_texts.append(self.__clean(doc))\n",
    "\n",
    "        return clean_texts\n",
    "\n",
    "    def __clean(self, doc: Doc) -> str:\n",
    "\n",
    "        tokens = []\n",
    "        # POS Tags removal\n",
    "        if self.__pos_to_remove:\n",
    "            for token in doc:\n",
    "                if token.pos_ not in self.__pos_to_remove:\n",
    "                    tokens.append(token)\n",
    "        else:\n",
    "            tokens = doc\n",
    "\n",
    "        # Remove Numbers\n",
    "        if self.__remove_numbers:\n",
    "            tokens = [\n",
    "                token for token in tokens if not (token.like_num or token.is_currency)\n",
    "            ]\n",
    "\n",
    "        # Remove Stopwords\n",
    "        if self.__remove_stop_words:\n",
    "            tokens = [token for token in tokens if not token.is_stop]\n",
    "        # remove unwanted tokens\n",
    "        tokens = [\n",
    "            token\n",
    "            for token in tokens\n",
    "            if not (\n",
    "                token.is_punct or token.is_space or token.is_quote or token.is_bracket\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Remove empty tokens\n",
    "        tokens = [token for token in tokens if token.text.strip() != \"\"]\n",
    "\n",
    "        # Lemmatize\n",
    "        if self.__lemmatize:\n",
    "            text = \" \".join([token.lemma_ for token in tokens])\n",
    "        else:\n",
    "            text = \" \".join([token.text for token in tokens])\n",
    "\n",
    "        if self.__remove_special:\n",
    "            # Remove non alphabetic characters\n",
    "            text = re.sub(r\"[^a-zA-Z\\']\", \" \", text)\n",
    "        # remove non-Unicode characters\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords = list(set(df[df['keyword'].notna()]['keyword']))\n",
    "keywords_lemmatized = [lemmatizer.lemmatize(word).lower() for word in keywords]\n",
    "filtered_df = df[df['keyword'].isna()]\n",
    "texts_without_keyword = filtered_df['text']\n",
    "ids_without_keyword = filtered_df['id']\n",
    "pattern = r'\\#\\w+'\n",
    "extracted_words = []\n",
    "\n",
    "for id, text in zip(ids_without_keyword, texts_without_keyword):\n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        word = re.search(r'\\w+', match).group()\n",
    "        if word in keywords or lemmatizer.lemmatize(word).lower() in keywords_lemmatized:\n",
    "            row_index = df.index[df['id'] == id].tolist()[0]\n",
    "            df['keyword'][row_index] = word\n",
    "            extracted_words.append(word)\n",
    "        else:\n",
    "            keywords = df.loc[df['keyword'].notna() & df['text'].str.contains(word), 'keyword'].tolist()\n",
    "            if len(keywords) == 1:\n",
    "                row_index = df.index[df['id'] == id].tolist()[0]\n",
    "                df['keyword'][row_index] = keywords[0]\n",
    "df.loc[df['keyword'].isna(), 'keyword'] = 'NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpaCyPreProcessor.download_spacy_model('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = SpaCyPreProcessor.load_model('en_core_web_trf')\n",
    "preprocessing_pipeline = SpaCyPreProcessor(spacy_model=spacy_model, remove_numbers=False, remove_special=True, remove_stop_words=True, lemmatize=True, use_gpu=True)\n",
    "\n",
    "df['cleaned_text'] = ''\n",
    "df['cleaned_keyword'] = ''\n",
    "\n",
    "for i in tqdm(df.index):\n",
    "    df['cleaned_text'][i] = preprocessing_pipeline.preprocess_text(df['text'][i])\n",
    "    df['cleaned_keyword'][i] = preprocessing_pipeline.preprocess_text(df['keyword'][i])\n",
    "df.loc[df['cleaned_keyword'] == '', 'cleaned_keyword'] = 'none'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(processed_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we search country for all locations to group tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(processed_data_path)\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "locations = df[df['location'].notna()]['location'].tolist()\n",
    "results = None\n",
    "arg = (locations, 0, len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geolocator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.geolocator = Nominatim(user_agent=f\"track\")\n",
    "\n",
    "    def get_country(self, location):\n",
    "        country = 'nan'\n",
    "        if location != 'nan':\n",
    "            geocode = self.geolocator.geocode(location)\n",
    "            if geocode:\n",
    "                country = geocode.address.split(',')[-1]\n",
    "        return country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_countries_found(location, country, countries_found = []):\n",
    "    check = False\n",
    "    for i in range(len(countries_found)):\n",
    "        if countries_found[i][\"country\"] == country:\n",
    "            countries_found[i][\"locations\"].append(location)\n",
    "            check = True\n",
    "    if check == False:\n",
    "        countries_found.append({\n",
    "            \"locations\": [location],\n",
    "            \"country\": country\n",
    "        })\n",
    "    return countries_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countries(locations, start, end, countries_found = None):\n",
    "    if countries_found == None:\n",
    "        countries_found = []\n",
    "    index = 0\n",
    "    cicle = None\n",
    "    try:\n",
    "        geolocator = Geolocator()\n",
    "        cicle = range(start, end) if end < len(locations) else range(start, len(locations))\n",
    "        for i in tqdm(cicle):\n",
    "            country = geolocator.get_country(str(locations[i]))\n",
    "            countries_found = update_countries_found(locations[i], country, countries_found)\n",
    "            index = i\n",
    "        return countries_found, end, end\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return countries_found, index, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./locations_checkpoint.json\"\n",
    "\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    with open(checkpoint_path, 'r') as openfile:\n",
    "        results = json.load(openfile)\n",
    "\n",
    "results = get_countries(*arg, results[0] if results else None)\n",
    "if int(results[-2]) != int(results[-1]):\n",
    "    arg = (locations, int(results[-2]), int(results[-1]))\n",
    "\n",
    "with open(checkpoint_path, 'w') as openfile:\n",
    "    json.dump(results, openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(checkpoint_path):\n",
    "    with open(checkpoint_path, 'r') as openfile:\n",
    "        results = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column named country\n",
    "countries = []\n",
    "for i in range(len(df['location'])):\n",
    "    countries_found = results[0]\n",
    "    location = str(df.iloc[i]['location'])\n",
    "    countries_found = str(None)\n",
    "    for country in countries_found:\n",
    "        if location in country['locations']:\n",
    "            country_found = country['country']\n",
    "    countries.append(country_found)\n",
    "df2 = df.assign(country=countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = list(set(df2['country'].tolist()))\n",
    "countries = [country.replace(' ', '_') for country in countries]\n",
    "text = \" \".join(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", ).generate(text)\n",
    "wordcloud.to_file(\"wordcloud_country.png\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(processed_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT/ROBERTA/DISTILBERT FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets\n",
    "!pip install accelerate -U -q\n",
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./outs ./wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "model = \"BERT\"\n",
    "dataset_path = './cleaned_train.csv'\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\n",
    "os.environ[\"WANDB_API_KEY\"] = \"b52c0b8bafc7f2f71a0cbd30c1b2d736a881787f\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "if model == \"BERT\":\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"BERT-Finetuning\"  # name your W&B project\n",
    "    model_id = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "    BERTModel = BertForSequenceClassification\n",
    "\n",
    "elif model == \"ROBERTA\":\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"ROBERTA-Finetuning\"  # name your W&B project\n",
    "    model_id = \"roberta-base\"\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_id)\n",
    "    BERTModel = RobertaForSequenceClassification\n",
    "\n",
    "elif model == \"DISTILBERT\":\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"DISTILBERT-Finetuning\"  # name your W&B project\n",
    "    model_id = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_id)\n",
    "    BERTModel = DistilBertForSequenceClassification\n",
    "\n",
    "else:\n",
    "    raise Exception(\"No valid model specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_opts = [\n",
    "    {\n",
    "        'path': '../data-to-train/preprocess_one.csv',\n",
    "        'feature': False,\n",
    "    },\n",
    "        {\n",
    "        'path': '../data-to-train/preprocess_two.csv',\n",
    "        'feature': False,\n",
    "    },\n",
    "        {\n",
    "        'path': '../data-to-train/preprocess_three.csv',\n",
    "        'feature': True,\n",
    "    },\n",
    "        {\n",
    "        'path': '../data-to-train/preprocess_four.csv',\n",
    "        'feature': True,\n",
    "    },\n",
    "]\n",
    "# Sweep configuration\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval/f1\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [8, 16, 32, 64]},\n",
    "        \"epochs\": {\"values\": [3]},\n",
    "        \"lr\": {\"max\": 1e-4, \"min\": 5e-5},\n",
    "        \"dataset\": {\"values\": [1, 2, 3, 4]},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7613/7613 [00:00<00:00, 45810.16 examples/s]\n",
      "Map: 100%|██████████| 7613/7613 [00:00<00:00, 47597.25 examples/s]\n",
      "Map: 100%|██████████| 7613/7613 [00:00<00:00, 14221.32 examples/s]\n",
      "Map: 100%|██████████| 7613/7613 [00:00<00:00, 13278.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(dataset_opt):\n",
    "    # Load dataset from CSV file\n",
    "    df = pd.read_csv(dataset_opt['path'])\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Tokenize text and keyword, and concatenate them\n",
    "    def tokenize_data(example):\n",
    "        text = example[\"cleaned_text\"]\n",
    "        keyword = example[\"cleaned_keyword\"]\n",
    "        if dataset_opt['feature']:\n",
    "            keyword += \",\" + str(example['hashtag_count'])\n",
    "            keyword += \",\" + str(example['url_count'])\n",
    "            keyword += \",\" + str(example['positive'])\n",
    "            keyword += \",\" + str(example['neutral'])\n",
    "            keyword += \",\" + str(example['negative'])\n",
    "            keyword += \",\" + str(example['ner_count'])\n",
    "        obj = tokenizer(text, keyword, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        obj[\"labels\"] = example[\"target\"]\n",
    "        return obj\n",
    "    \n",
    "    # Apply tokenization to all examples in the dataset\n",
    "    dataset = dataset.map(tokenize_data, remove_columns=dataset.column_names)\n",
    "    dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for dataset_opt in dataset_opts:\n",
    "    datasets.append(prepare_dataset(dataset_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return { \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1 }\n",
    "\n",
    "def objective(config):\n",
    "    model = BERTModel.from_pretrained(model_id, num_labels=2)\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=config.epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        output_dir=\"./outs\",\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=1,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[config.dataset - 1]['train'],\n",
    "        eval_dataset=datasets[config.dataset - 1]['test'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, None),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    # Search hyperparameters\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    objective(wandb.config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=os.environ[\"WANDB_PROJECT\"])\n",
    "wandb.agent(sweep_id, function=main, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers@main\n",
    "!pip install -q causal-conv1d>=1.2.0 mamba-ssm trl peft datasets\n",
    "!pip install accelerate -U -q\n",
    "!git clone https://github.com/getorca/mamba_for_sequence_classification.git\n",
    "!pip install ./mamba_for_sequence_classification/. -q\n",
    "!pip install bnb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer\n",
    "from hf_mamba_classification import MambaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "dataset_path = \"../input/cleaned-train-csv/cleaned_train.csv\"\n",
    "wandb.login(key=\"b52c0b8bafc7f2f71a0cbd30c1b2d736a881787f\")\n",
    "os.environ[\"WANDB_PROJECT\"] = \"MAMBA-Finetuning\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\n",
    "model_id = \"state-spaces/mamba-130m-hf\"\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_opts = [\n",
    "    {\n",
    "        'path': '../data-to-train/preprocess_one.csv',\n",
    "        'feature': False,\n",
    "    },\n",
    "        {\n",
    "        'path': '../data-to-train/preprocess_two.csv',\n",
    "        'feature': False,\n",
    "    },\n",
    "        {\n",
    "        'path': '../data-to-train/preprocess_three.csv',\n",
    "        'feature': True,\n",
    "    },\n",
    "        {\n",
    "        'path': '../data-to-train/preprocess_four.csv',\n",
    "        'feature': True,\n",
    "    },\n",
    "]\n",
    "# Sweep configuration\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval/f1\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [8, 16, 32, 64]},\n",
    "        \"epochs\": {\"values\": [3]},\n",
    "        \"lr\": {\"max\": 1e-4, \"min\": 5e-5},\n",
    "        \"dataset\": {\"values\": [1, 2, 3, 4]},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_opt):\n",
    "    # Load dataset from CSV file\n",
    "    df = pd.read_csv(dataset_opt['path'])\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Tokenize text and keyword, and concatenate them\n",
    "    def tokenize_data(example):\n",
    "        text = example[\"cleaned_text\"]\n",
    "        keyword = example[\"cleaned_keyword\"]\n",
    "        if dataset_opt['feature']:\n",
    "            keyword += \",\" + str(example['hashtag_count'])\n",
    "            keyword += \",\" + str(example['url_count'])\n",
    "            keyword += \",\" + str(example['positive'])\n",
    "            keyword += \",\" + str(example['neutral'])\n",
    "            keyword += \",\" + str(example['negative'])\n",
    "            keyword += \",\" + str(example['ner_count'])\n",
    "        obj = tokenizer(text, keyword, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        obj[\"labels\"] = example[\"target\"]\n",
    "        return obj\n",
    "    \n",
    "    # Apply tokenization to all examples in the dataset\n",
    "    dataset = dataset.map(tokenize_data, remove_columns=dataset.column_names)\n",
    "    dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for dataset_opt in dataset_opts:\n",
    "    datasets.append(prepare_dataset(dataset_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return { \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1 }\n",
    "\n",
    "def objective(config):\n",
    "    model = MambaForSequenceClassification.from_pretrained(\n",
    "        model_id, \n",
    "        num_labels=2, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id,\n",
    "        use_cache=False  # This needs to be passed when using eval and training Mamba for sequence classification otherwise it will raise an error\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"mamba_imdb_classification\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        num_train_epochs=config.epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # eval_steps=1000,\n",
    "        save_strategy=\"epoch\",\n",
    "        # load_best_model_at_end=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        # optim='paged_adamw_8bit',\n",
    "        # push_to_hub=True,  \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[config.dataset - 1]['train'],\n",
    "        eval_dataset=datasets[config.dataset - 1]['test'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    objective(wandb.config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=os.environ[\"WANDB_PROJECT\"])\n",
    "\n",
    "wandb.agent(sweep_id, function=main, count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
